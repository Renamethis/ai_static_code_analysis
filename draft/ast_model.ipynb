{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test2\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import sys\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load your CSV dataset\n",
    "# The dataset should have two columns: \"vulnerable_code\" and \"fixed_code\"\n",
    "dataset = load_dataset(\"json\", data_files=\"MSR_data_cleaned.json\")\n",
    "# Split the dataset into training and validation sets\n",
    "#dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "print('test')\n",
    "# Initialize the tokenizer for Llama\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyPixel/Llama-2-7B-bf16-sharded\")  # Replace with your desired Llama model\n",
    "print('test2')\n",
    "# Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    print(len(example))\n",
    "    # Prepare input as \"Fix the vulnerability: [vulnerable_code]\" and target as \"fixed_code\"\n",
    "    example[\"input_text\"] = f\"Fix the vulnerability: {example['func_before']}\"\n",
    "    example[\"target_text\"] = example[\"vul_func_with_fix\"]\n",
    "    \n",
    "    # Tokenize input and target\n",
    "    input_encodings = tokenizer(example[\"input_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    target_encodings = tokenizer(example[\"target_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Return the encodings as input_ids and labels\n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "        \"labels\": target_encodings[\"input_ids\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0458cdaafc477ca120c7dfff0e8543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188636\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'func_before'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set the dataset format for PyTorch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=552'>553</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=553'>554</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=554'>555</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=555'>556</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=556'>557</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=557'>558</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=558'>559</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=559'>560</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=560'>561</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=561'>562</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3048'>3049</a>\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3049'>3050</a>\u001b[0m     \u001b[39mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3050'>3051</a>\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3051'>3052</a>\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3052'>3053</a>\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3053'>3054</a>\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3054'>3055</a>\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3055'>3056</a>\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3056'>3057</a>\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3458\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3453'>3454</a>\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3454'>3455</a>\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3455'>3456</a>\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3456'>3457</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3457'>3458</a>\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3458'>3459</a>\u001b[0m         batch,\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3459'>3460</a>\u001b[0m         indices,\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3460'>3461</a>\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3461'>3462</a>\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3462'>3463</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3463'>3464</a>\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3464'>3465</a>\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3465'>3466</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3466'>3467</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3317'>3318</a>\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3318'>3319</a>\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3319'>3320</a>\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3320'>3321</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3321'>3322</a>\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3322'>3323</a>\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py?line=3323'>3324</a>\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(example))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Prepare input as \"Fix the vulnerability: [vulnerable_code]\" and target as \"fixed_code\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFix the vulnerability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfunc_before\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvul_func_with_fix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Tokenize input and target\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:277\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=275'>276</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=276'>277</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[key]\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=277'>278</a>\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys_to_format:\n\u001b[1;32m    <a href='file:///home/ivan/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py?line=278'>279</a>\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'func_before'"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "print('test3')\n",
    "# Set the dataset format for PyTorch\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
