{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = None\n",
    "with open(\"MSR_data_cleaned.json\", \"r\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': '0', 'Access Gained': 'None', 'Attack Origin': 'Remote', 'Authentication Required': 'Single system', 'Availability': 'Partial', 'CVE ID': 'CVE-2015-8467', 'CVE Page': 'https://www.cvedetails.com/cve/CVE-2015-8467/', 'CWE ID': 'CWE-264', 'Complexity': 'Medium', 'Confidentiality': 'Partial', 'Integrity': 'Partial', 'Known Exploits': '', 'Publish Date': '2015-12-29', 'Score': '6.0', 'Summary': 'The samldb_check_user_account_control_acl function in dsdb/samdb/ldb_modules/samldb.c in Samba 4.x before 4.1.22, 4.2.x before 4.2.7, and 4.3.x before 4.3.3 does not properly check for administrative privileges during creation of machine accounts, which allows remote authenticated users to bypass intended access restrictions by leveraging the existence of a domain with both a Samba DC and a Windows DC, a similar issue to CVE-2015-2535.', 'Update Date': '2016-12-30', 'Vulnerability Classification': 'Bypass', 'add_lines': '0', 'codeLink': 'https://git.samba.org/?p=samba.git;a=commit;h=b000da128b5fb519d2d3f2e7fd20e4a25b7dae7d', 'commit_id': 'b000da128b5fb519d2d3f2e7fd20e4a25b7dae7d', 'commit_message': '', 'del_lines': '0', 'file_name': '', 'files_changed': '', 'func_after': 'static bool check_rodc_critical_attribute(struct ldb_message *msg)\\n{\\n\\tuint32_t schemaFlagsEx, searchFlags, rodc_filtered_flags;\\n\\n\\tschemaFlagsEx = ldb_msg_find_attr_as_uint(msg, \"schemaFlagsEx\", 0);\\n\\tsearchFlags = ldb_msg_find_attr_as_uint(msg, \"searchFlags\", 0);\\n\\trodc_filtered_flags = (SEARCH_FLAG_RODC_ATTRIBUTE\\n\\t\\t\\t      | SEARCH_FLAG_CONFIDENTIAL);\\n\\n\\tif ((schemaFlagsEx & SCHEMA_FLAG_ATTR_IS_CRITICAL) &&\\n\\t\\t((searchFlags & rodc_filtered_flags) == rodc_filtered_flags)) {\\n\\t\\treturn true;\\n\\t} else {\\n\\t\\treturn false;\\n\\t}\\n}\\n', 'func_before': 'static bool check_rodc_critical_attribute(struct ldb_message *msg)\\n{\\n\\tuint32_t schemaFlagsEx, searchFlags, rodc_filtered_flags;\\n\\n\\tschemaFlagsEx = ldb_msg_find_attr_as_uint(msg, \"schemaFlagsEx\", 0);\\n\\tsearchFlags = ldb_msg_find_attr_as_uint(msg, \"searchFlags\", 0);\\n\\trodc_filtered_flags = (SEARCH_FLAG_RODC_ATTRIBUTE\\n\\t\\t\\t      | SEARCH_FLAG_CONFIDENTIAL);\\n\\n\\tif ((schemaFlagsEx & SCHEMA_FLAG_ATTR_IS_CRITICAL) &&\\n\\t\\t((searchFlags & rodc_filtered_flags) == rodc_filtered_flags)) {\\n\\t\\treturn true;\\n\\t} else {\\n\\t\\treturn false;\\n\\t}\\n}\\n', 'lang': 'C', 'lines_after': '', 'lines_before': '', 'parentID': 'a819d2b440aafa3138d95ff6e8b824da885a70e9', 'patch': '@@ -1558,12 +1558,15 @@ static int samldb_check_user_account_control_acl(struct samldb_ctx *ac,\\n        struct security_token *user_token;\\n        struct security_descriptor *domain_sd;\\n        struct ldb_dn *domain_dn = ldb_get_default_basedn(ldb_module_get_ctx(ac->module));\\n+       struct ldb_context *ldb = ldb_module_get_ctx(ac->module);\\n        const struct uac_to_guid {\\n                uint32_t uac;\\n+               uint32_t priv_to_change_from;\\n                const char *oid;\\n                const char *guid;\\n                enum sec_privilege privilege;\\n                bool delete_is_privileged;\\n+               bool admin_required;\\n                const char *error_string;\\n        } map[] = {\\n                {\\n@@ -1591,6 +1594,16 @@ static int samldb_check_user_account_control_acl(struct samldb_ctx *ac,\\n                        .guid = GUID_DRS_DS_INSTALL_REPLICA,\\n                        .error_string = \"Adding the UF_PARTIAL_SECRETS_ACCOUNT bit in userAccountControl requires the DS-Install-Replica right that was not given on the Domain object\"\\n                },\\n+               {\\n+                       .uac = UF_WORKSTATION_TRUST_ACCOUNT,\\n+                       .priv_to_change_from = UF_NORMAL_ACCOUNT,\\n+                       .error_string = \"Swapping UF_NORMAL_ACCOUNT to UF_WORKSTATION_TRUST_ACCOUNT requires the user to be a member of the domain admins group\"\\n+               },\\n+               {\\n+                       .uac = UF_NORMAL_ACCOUNT,\\n+                       .priv_to_change_from = UF_WORKSTATION_TRUST_ACCOUNT,\\n+                       .error_string = \"Swapping UF_WORKSTATION_TRUST_ACCOUNT to UF_NORMAL_ACCOUNT requires the user to be a member of the domain admins group\"\\n+               },\\n                {\\n                        .uac = UF_INTERDOMAIN_TRUST_ACCOUNT,\\n                        .oid = DSDB_CONTROL_PERMIT_INTERDOMAIN_TRUST_UAC_OID,\\n@@ -1643,7 +1656,7 @@ static int samldb_check_user_account_control_acl(struct samldb_ctx *ac,\\n                return ldb_module_operr(ac->module);\\n        }\\n \\n-       ret = dsdb_get_sd_from_ldb_message(ldb_module_get_ctx(ac->module),\\n+       ret = dsdb_get_sd_from_ldb_message(ldb,\\n                                           ac, res->msgs[0], &domain_sd);\\n \\n        if (ret != LDB_SUCCESS) {\\n@@ -1670,12 +1683,19 @@ static int samldb_check_user_account_control_acl(struct samldb_ctx *ac,\\n                                if (have_priv == false) {\\n                                        ret = LDB_ERR_INSUFFICIENT_ACCESS_RIGHTS;\\n                                }\\n-                       } else {\\n+                       } else if (map[i].priv_to_change_from & user_account_control_old) {\\n+                               bool is_admin = security_token_has_builtin_administrators(user_token);\\n+                               if (is_admin == false) {\\n+                                       ret = LDB_ERR_INSUFFICIENT_ACCESS_RIGHTS;\\n+                               }\\n+                       } else if (map[i].guid) {\\n                                ret = acl_check_extended_right(ac, domain_sd,\\n                                                               user_token,\\n                                                               map[i].guid,\\n                                                               SEC_ADS_CONTROL_ACCESS,\\n                                                               sid);\\n+                       } else {\\n+                               ret = LDB_SUCCESS;\\n                        }\\n                        if (ret != LDB_SUCCESS) {\\n                                break;', 'project': 'samba', 'project_after': 'https://git.samba.org/?p=samba.git;a=blob;f=source4/dsdb/samdb/ldb_modules/samldb.c;h=df285d91485ba8393d368ddf6328957d26ff57dd;hb=df285d91485ba8393d368ddf6328957d26ff57dd', 'project_before': 'https://git.samba.org/?p=samba.git;a=blob;f=source4/dsdb/samdb/ldb_modules/samldb.c;h=e3a7db27aa9c4b2ea5fba2f4f91b87d90c502e98;hb=e3a7db27aa9c4b2ea5fba2f4f91b87d90c502e98', 'vul': '0', 'vul_func_with_fix': 'static bool check_rodc_critical_attribute(struct ldb_message *msg)\\n{\\n\\tuint32_t schemaFlagsEx, searchFlags, rodc_filtered_flags;\\n\\n\\tschemaFlagsEx = ldb_msg_find_attr_as_uint(msg, \"schemaFlagsEx\", 0);\\n\\tsearchFlags = ldb_msg_find_attr_as_uint(msg, \"searchFlags\", 0);\\n\\trodc_filtered_flags = (SEARCH_FLAG_RODC_ATTRIBUTE\\n\\t\\t\\t      | SEARCH_FLAG_CONFIDENTIAL);\\n\\n\\tif ((schemaFlagsEx & SCHEMA_FLAG_ATTR_IS_CRITICAL) &&\\n\\t\\t((searchFlags & rodc_filtered_flags) == rodc_filtered_flags)) {\\n\\t\\treturn true;\\n\\t} else {\\n\\t\\treturn false;\\n\\t}\\n}\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(list(data.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_before\n",
    "vul_func_with_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "from tree_sitter import Language, Parser\n",
    "from subprocess import check_output\n",
    "from tree_sitter_cpp import language\n",
    "\n",
    "CPP_LANGUAGE = Language(language())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def ast_to_graph(ast):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    node_features = []\n",
    "\n",
    "    def traverse_tree(node, parent_id=None):\n",
    "        node_id = len(nodes)\n",
    "        nodes.append(node)\n",
    "        node_features.append(node.type)  # Encode node type as a feature\n",
    "\n",
    "        if parent_id is not None:\n",
    "            edges.append((parent_id, node_id))  # Parent-child edge\n",
    "\n",
    "        for child in node.children:\n",
    "            traverse_tree(child, node_id)\n",
    "\n",
    "    traverse_tree(ast.root_node)\n",
    "    return nodes, edges, node_features\n",
    "\n",
    "def parse_cpp(code):\n",
    "    # Initialize parser and set language to C++\n",
    "    parser = Parser(CPP_LANGUAGE)\n",
    "    #print(dir(Language))\n",
    "    #CPP_LANGUAGE = Language('build/my-languages.so', 'cpp')\n",
    "    # Parse the code and generate the AST\n",
    "    tree = parser.parse(bytes(code, 'utf8'))\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load your CSV dataset\n",
    "# The dataset should have two columns: \"vulnerable_code\" and \"fixed_code\"\n",
    "dataset = load_dataset(\"json\", data_files=\"MSR_data_cleaned.json\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Initialize the tokenizer for Llama\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")  # Replace with your desired Llama model\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(example):\n",
    "    # Prepare input as \"Fix the vulnerability: [vulnerable_code]\" and target as \"fixed_code\"\n",
    "    example[\"input_text\"] = f\"Fix the vulnerability: {example['func_before']}\"\n",
    "    example[\"target_text\"] = example[\"vul_func_with_fix\"]\n",
    "    \n",
    "    # Tokenize input and target\n",
    "    input_encodings = tokenizer(example[\"input_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    target_encodings = tokenizer(example[\"target_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Return the encodings as input_ids and labels\n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "        \"labels\": target_encodings[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the dataset format for PyTorch\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the pre-trained Llama model for fine-tuning\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-vuln-fixer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
