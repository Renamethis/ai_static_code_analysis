{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_scheduler\n",
    "from datasets import load_dataset\n",
    "import tree_sitter\n",
    "from tree_sitter import Language, Parser\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tree_sitter_java import language\n",
    "\n",
    "JAVA = Language(language())\n",
    "\n",
    "parser = Parser(JAVA)\n",
    "\n",
    "import os\n",
    "\n",
    "# Step 1: Setup tree-sitter for AST parsing (example for Python code)\n",
    "\n",
    "# Step 2: Load CodeXGLUE Code Refinement Dataset\n",
    "dataset = load_dataset(\"google/code_x_glue_cc_code_refinement\", 'medium', split='train[:40%]')\n",
    "val_dataset = load_dataset(\"google/code_x_glue_cc_code_refinement\", 'medium', split='validation[:2%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.roberta.configuration_roberta.RobertaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m gpt_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m gpt_tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m gpt_tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCodeRefinementDataset\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, unix_tokenizer, gpt_tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:567\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.roberta.configuration_roberta.RobertaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize UniXcoder Model and Tokenizer\n",
    "model_name = \"microsoft/unixcoder-base\"  # Replace with the correct UniXcoder model if different\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "class CodeRefinementDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, unix_tokenizer, gpt_tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.unix_tokenizer = unix_tokenizer\n",
    "        self.gpt_tokenizer = gpt_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        buggy_code = item['original_string']\n",
    "        fixed_code = item['fixed_string']\n",
    "\n",
    "        # Encode input and target\n",
    "        input_enc = self.unix_tokenizer(buggy_code, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_enc = self.gpt_tokenizer(fixed_code, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        # Fake AST features for demo (replace with real AST)\n",
    "        ast_features = self.extract_ast_features(buggy_code)  # (seq_len, ast_feature_dim)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': input_enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': target_enc['input_ids'].squeeze(0),\n",
    "            'labels': target_enc['input_ids'].squeeze(0),\n",
    "            'ast_features': ast_features\n",
    "        }\n",
    "\n",
    "    def extract_ast_features(self, code):\n",
    "        \"\"\"\n",
    "        Extract AST features using tree-sitter.\n",
    "        For simplicity, return a placeholder (e.g., node count or depth).\n",
    "        In practice, you can encode AST paths or node types into embeddings.\n",
    "        \"\"\"\n",
    "        tree = parser.parse(bytes(code, \"utf8\"))\n",
    "        root_node = tree.root_node\n",
    "        \n",
    "        # Example: Count number of nodes as a simple feature\n",
    "        def count_nodes(node):\n",
    "            count = 1\n",
    "            for child in node.children:\n",
    "                count += count_nodes(child)\n",
    "            return count\n",
    "        \n",
    "        node_count = count_nodes(root_node)\n",
    "        # Placeholder: Return a simple feature vector (expand this for real use)\n",
    "        return [node_count, len(code.splitlines())]\n",
    "\n",
    "# Step 5: DataLoader Setup\n",
    "train_dataset = CodeRefinementDataset(dataset, tokenizer)\n",
    "val_dataset = CodeRefinementDataset(val_dataset, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "class UniXcoderWithASTEncoder(nn.Module):\n",
    "    def __init__(self, base_model, ast_feature_dim=2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.ast_projection = nn.Linear(ast_feature_dim, base_model.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, ast_features=None):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden = outputs.last_hidden_state\n",
    "        if ast_features is not None:\n",
    "            ast_embeds = self.ast_projection(ast_features)\n",
    "            hidden = hidden + ast_embeds\n",
    "        return hidden, attention_mask\n",
    "\n",
    "\n",
    "class UniXcoderSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder_name=\"gpt2\"):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(decoder_name)\n",
    "        self.decoder.config.pad_token_id = self.decoder.config.eos_token_id\n",
    "        self.enc_to_dec = nn.Linear(\n",
    "            encoder.base_model.config.hidden_size,\n",
    "            self.decoder.config.hidden_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids, ast_features=None, labels=None):\n",
    "        encoder_hidden_states, encoder_attention_mask = self.encoder(input_ids, attention_mask, ast_features)\n",
    "        encoder_hidden_states = self.enc_to_dec(encoder_hidden_states)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "def generate_with_beam_search(\n",
    "    model, input_ids, attention_mask, ast_features, tokenizer,\n",
    "    max_length=64, num_beams=4\n",
    "):\n",
    "    encoder_hidden_states, encoder_attention_mask = model.encoder(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        ast_features=ast_features\n",
    "    )\n",
    "    encoder_hidden_states = model.enc_to_dec(encoder_hidden_states)\n",
    "\n",
    "    # Use decoder's generate function (inherited from PreTrainedModel)\n",
    "    generated_ids = model.decoder.generate(\n",
    "        input_ids=None,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=tokenizer.bos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model\n",
    "encoder = UniXcoderWithASTEncoder(model)\n",
    "model = UniXcoderSeq2Seq(encoder).to(device)\n",
    "\n",
    "# Prepare data\n",
    "train_dataset = CodeRefinementDataset(dataset, tokenizer, gpt_tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        ast_features = batch['ast_features'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "            ast_features=ast_features\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {total_loss / len(train_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
